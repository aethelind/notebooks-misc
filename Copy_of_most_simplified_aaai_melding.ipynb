{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqPMy/9pC76jqMJYoslZtD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aethelind/notebooks-misc/blob/main/Copy_of_most_simplified_aaai_melding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## submodular.py"
      ],
      "metadata": {
        "id": "odwsZHjsYcW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YvmZ_k9WwJpz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.sparse\n",
        "import scipy.linalg\n",
        "\n",
        "\n",
        "class ContinuousOptimizer(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    pytorch module for differentiable submodular maximization. The forward pass \n",
        "    computes the optimal x for given parameters. The backward pass differentiates \n",
        "    that optimal x wrt the parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, params):\n",
        "        \"\"\"\n",
        "        Computes the optimal x using the supplied optimizer. \n",
        "        \"\"\"\n",
        "        with torch.enable_grad():\n",
        "            x = optimize_coverage_multilinear(P=params, verbose=True, k=2, c=0.95, minibatch_size=None)\n",
        "        ctx.save_for_backward(params, x) \n",
        "        return x.data\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Differentiates the optimal x returned by the forward pass with respect\n",
        "        to the ratings matrix that was given as input.\n",
        "        \"\"\"\n",
        "        params, x = ctx.saved_tensors \n",
        "        xgrad = x.grad.data\n",
        "        dxdr = ContinuousOptimizer.get_dxdr(x.data.detach().numpy(), -xgrad.detach().numpy(), params.detach().numpy(), dgrad_coverage, 0.95)\n",
        "        dxdr_t = torch.from_numpy(np.transpose(dxdr))\n",
        "        out = torch.mm(dxdr_t.float(), grad_output.view(len(x), 1))\n",
        "        return out.view_as(params)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dxdr(x, grad, params, get_dgradf_dparams, max_x):\n",
        "        '''\n",
        "        Returns the derivative of the optimal solution in the region around x in \n",
        "        terms of the rating matrix r. \n",
        "\n",
        "        x: an optimal solution\n",
        "\n",
        "        grad: df/dx at x\n",
        "\n",
        "        params: the current parameter settings\n",
        "        '''\n",
        "        n = len(x)\n",
        "        # first get the optimal dual variables via the KKT conditions\n",
        "        # dual variable for constraint sum(x) <= k\n",
        "        if np.logical_and(x > 0, x < max_x).any():\n",
        "            lambda_sum = np.mean(grad[np.logical_and(x > 0, x < max_x)])\n",
        "        else:\n",
        "            lambda_sum = 0\n",
        "        # dual variable for constraint x <= max_x\n",
        "        lambda_upper = []\n",
        "        # dual variable for constraint x >= 0\n",
        "        lambda_lower = []\n",
        "        for i in range(n):\n",
        "            if np.abs(x[i] - max_x) < 0.000001:\n",
        "                lambda_upper.append(grad[i] - lambda_sum)\n",
        "            else:\n",
        "                lambda_upper.append(0)\n",
        "            if x[i] > 0:\n",
        "                lambda_lower.append(0)\n",
        "            else:\n",
        "                lambda_lower.append(grad[i] - lambda_sum)\n",
        "        # number of constraints\n",
        "        m = 2*n + 1\n",
        "        # collect value of dual variables\n",
        "        lam = np.zeros((m))\n",
        "        lam[0] = lambda_sum\n",
        "        lam[1:(n+1)] = lambda_upper\n",
        "        lam[n+1:] = lambda_lower\n",
        "        diag_lambda = np.matrix(np.diag(lam))\n",
        "        # collect value of constraints\n",
        "        g = np.zeros((m))\n",
        "        # TODO: replace the second x.sum() with k so that this is actually generally correct\n",
        "        g[0] = x.sum() - x.sum()\n",
        "        g[1:(n+1)] = x - max_x\n",
        "        g[n+1:] = -x\n",
        "        diag_g = np.matrix(np.diag(g))\n",
        "        # gradient of constraints wrt x\n",
        "        dgdx = np.zeros((m, n))\n",
        "        # gradient of constraint sum(x) <= k\n",
        "        dgdx[0, :] = 1\n",
        "        # gradient of constraints x <= 1\n",
        "        for i in range(1, n+1):\n",
        "            dgdx[i, i-1] = 1\n",
        "        # gradient of constraints x >= 0 <--> -x <= 0\n",
        "        for i in range(n+1, m):\n",
        "            dgdx[i, i-(n+1)] = -1\n",
        "        dgdx = np.matrix(dgdx)\n",
        "        # the Hessian matrix -- all zeros for now\n",
        "        H = np.matrix(np.zeros((n, n)))\n",
        "        # coefficient matrix for the linear system\n",
        "        A = np.bmat([[H, np.transpose(dgdx)], [diag_lambda*dgdx, diag_g]])\n",
        "        # add 0.01*I to improve conditioning\n",
        "        A = A + 0.01*np.eye(n+m)\n",
        "        # RHS of the linear system, mostly partial derivative of grad f wrt params\n",
        "        dgradf_dparams = get_dgradf_dparams(x, params)\n",
        "        reshaped = np.zeros(\n",
        "            (dgradf_dparams.shape[0], dgradf_dparams.shape[1]*dgradf_dparams.shape[2]))\n",
        "        for i in range(n):\n",
        "            reshaped[i] = dgradf_dparams[i].flatten()\n",
        "        b = np.bmat([[reshaped], [np.zeros((m, reshaped.shape[1]))]])\n",
        "        # solution to the system\n",
        "        derivatives = sp.linalg.solve(A, b)\n",
        "        if np.isnan(derivatives).any():\n",
        "            print('report')\n",
        "            print(np.isnan(A).any())\n",
        "            print(np.isnan(b).any())\n",
        "            print(np.isnan(dgdx).any())\n",
        "            print(np.isnan(diag_lambda).any())\n",
        "            print(np.isnan(diag_g).any())\n",
        "            print(np.isnan(dgradf_dparams).any())\n",
        "        # first n are derivatives of primal variables\n",
        "        derivatives = derivatives[:n]\n",
        "        return derivatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coverage.py"
      ],
      "metadata": {
        "id": "N8c85RN6YfLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "\n",
        "\n",
        "@jit\n",
        "def gradient_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    m = len(x)\n",
        "    grad = np.zeros(m, dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:, i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        for j in range(m):\n",
        "            grad[j] += P[j, i] * p_all_fail/p_fail[j]\n",
        "    return grad\n",
        "\n",
        "\n",
        "@jit\n",
        "def objective_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    total = 0\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:, i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        total += (1 - p_all_fail)\n",
        "    return total\n",
        "\n",
        "\n",
        "class CoverageInstanceMultilinear(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Represents a coverage instance with given coverage probabilities\n",
        "    P. Forward pass computes the objective value (if evaluate_forward\n",
        "    is true). Backward computes the gradients w.r.t. decision variables x.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, P):\n",
        "        ctx.save_for_backward(x, P)\n",
        "        out = objective_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        return torch.tensor(out).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        x, P = ctx.saved_tensors\n",
        "        grad = gradient_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        return torch.from_numpy(grad).float()*grad_in.float(), None\n",
        "\n",
        "\n",
        "def optimize_coverage_multilinear(P, verbose=True, k=10, c=1., minibatch_size=None):\n",
        "    '''\n",
        "    Run some variant of SGD for the coverage problem with given \n",
        "    coverage probabilities P.\n",
        "    '''\n",
        "    # decision variables\n",
        "    x = torch.zeros(P.shape[0], requires_grad=True)\n",
        "    # set up the optimizer\n",
        "    optimizer = torch.optim.SGD([x], momentum=0.9, lr=0.1, nesterov=True)\n",
        "    # take projected stochastic gradient steps\n",
        "    for t in range(20):\n",
        "        # objective which will provide gradient evaluations\n",
        "        loss = -CoverageInstanceMultilinear.apply(x, P)\n",
        "        #if verbose:\n",
        "            #print(t, -loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        x.data = torch.from_numpy(project_uniform_matroid_boundary(x.data.numpy(), k, 1/c)).float()\n",
        "    return x\n",
        "\n",
        "\n",
        "@jit\n",
        "def dgrad_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    m = len(x)\n",
        "    dgrad = np.zeros((m, m, n), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:,i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        for j in range(m):\n",
        "            for k in range(m):\n",
        "                if j == k:\n",
        "                    dgrad[j, k, i] = p_all_fail/p_fail[j]\n",
        "                else:\n",
        "                    dgrad[j, k, i] = -x[k] * P[j, i] * p_all_fail/(p_fail[j] * p_fail[k])\n",
        "    return dgrad"
      ],
      "metadata": {
        "id": "SrYMOsSPwM5p"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "xWd82ffpYkF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_uniform_matroid_boundary(x, k, c=1):\n",
        "    '''\n",
        "    Exact projection algorithm of Karimi et al. This is the projection implementation\n",
        "    that should be used now.\n",
        "    \n",
        "    Projects x onto the set {y: 0 <= y <= 1/c, ||y||_1 = k}\n",
        "    '''\n",
        "    import numpy as np\n",
        "    k *= c\n",
        "    n = len(x)\n",
        "    x = x.copy()\n",
        "    alpha_upper = x/c\n",
        "    alpha_lower = (x*c - 1)/c**2\n",
        "    S = []\n",
        "    S.extend(alpha_lower)\n",
        "    S.extend(alpha_upper)\n",
        "    S.sort()\n",
        "    S = np.unique(S)\n",
        "    h = n\n",
        "    alpha = min(S) - 1\n",
        "    m = 0\n",
        "    for i in range(len(S)):\n",
        "        hprime = h + (S[i] - alpha)*m\n",
        "        if hprime < k and k <= h:\n",
        "            alphastar = (S[i] - alpha)*(h - k)/(h - hprime) + alpha\n",
        "            result = np.zeros((n))\n",
        "            for j in range(n):\n",
        "                if alpha_lower[j] > alphastar:\n",
        "                    result[j] = 1./c\n",
        "                elif alpha_upper[j] >= alphastar:\n",
        "                    result[j] = x[j] - alphastar*c\n",
        "            return result\n",
        "        m -= (alpha_lower == S[i]).sum()*(c**2)\n",
        "        m += (alpha_upper == S[i]).sum()*(c**2)\n",
        "        h = hprime\n",
        "        alpha = S[i]\n",
        "    raise Exception('projection did not terminate')"
      ],
      "metadata": {
        "id": "cl6N0_01wOyP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear out directory\n",
        "!rm -rf *\n",
        "# Download data_decisions_benchmarks.zip and unzip diverse_recommendation_data.pickle\n",
        "!curl https://bryanwilder.github.io/files/data_decisions_benchmarks.zip | jar xv benchmarks_release/diverse_recommendation_data.pickle\n",
        "# Move diverse_recommendation_data.pickle to current directory\n",
        "!mv benchmarks_release/diverse_recommendation_data.pickle .\n",
        "# Remove empty directory\n",
        "!rm -rf benchmarks_release\n",
        "# Download hetrec2011-movielens-2k-v2.zip and unzip movie_actors.dat and user_ratedmovies.dat\n",
        "!curl https://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip | jar xv movie_actors.dat user_ratedmovies.dat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GstNrrAwT_Y",
        "outputId": "53f663df-3ee1-46bd-fbe8-219b1d0b652b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 83.0M  100 83.0M    0     0  24.4M      0  0:00:03  0:00:03 --:--:-- 24.4M\n",
            " inflated: benchmarks_release/diverse_recommendation_data.pickle\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 inflated: movie_actors.dat\n",
            " 46 17.9M   46 8608k    0     0  9196k      0  0:00:02 --:--:--  0:00:02 9186k inflated: user_ratedmovies.dat\n",
            "100 17.9M  100 17.9M    0     0  14.2M      0  0:00:01  0:00:01 --:--:-- 14.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## recommendation_nn_decision.py\n",
        "## movie problem\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "num_layers = 1\n",
        "activation = 'relu'\n",
        "#k = 20\n",
        "#use_hessian = False\n",
        "num_iters = 100\n",
        "instance_sizes = [100]\n",
        "learning_rate = 1e-4\n",
        "\n",
        "Ps = {}\n",
        "data = {}\n",
        "f_true = {}\n",
        "for num_items in instance_sizes:\n",
        "    with open('diverse_recommendation_data' + '.pickle', 'rb') as f:\n",
        "        # Ps_size takes on the actors 100x500 and data_size takes the users 100x2113\n",
        "        Ps_size, data_size = pickle.load(f)\n",
        "\n",
        "    num_targets = Ps_size[0].shape[1]\n",
        "    num_features = data_size[0].shape[1]\n",
        "    Ps[num_items] = [torch.from_numpy(P).long() for P in Ps_size]\n",
        "    data[num_items] = [torch.from_numpy(x).float() for x in data_size]\n",
        "    w = np.ones(num_targets, dtype=np.float32)\n",
        "    f_true[num_items] = [(P, w) for P in Ps[num_items]]\n",
        "\n",
        "\n",
        "num_repetitions = 0\n",
        "\n",
        "train = {}\n",
        "test = {}\n",
        "for size in instance_sizes:\n",
        "    with open('diverse_recommendation_data' + '.pickle', 'rb') as f:\n",
        "        train[size], test[size] = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "uBQ7Tb0GV0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "mrcmbbiPYXeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals = np.zeros((num_repetitions+30, len(instance_sizes), len(instance_sizes)))\n",
        "\n",
        "for idx in range(num_repetitions, num_repetitions+1): #+ 30):\n",
        "\n",
        "    intermediate_size = 200\n",
        "    def make_fc():\n",
        "        if num_layers > 1:\n",
        "            if activation == 'relu':\n",
        "                activation_fn = nn.ReLU\n",
        "            elif activation == 'sigmoid':\n",
        "                activation_fn = nn.Sigmoid\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    'Invalid activation function: ' + str(activation))\n",
        "            net_layers = [\n",
        "                nn.Linear(num_features, intermediate_size), activation_fn()]\n",
        "            for hidden in range(num_layers-2):\n",
        "                net_layers.append(\n",
        "                    nn.Linear(intermediate_size, intermediate_size))\n",
        "                net_layers.append(activation_fn())\n",
        "            net_layers.append(nn.Linear(intermediate_size, num_targets))\n",
        "            net_layers.append(nn.Sigmoid())\n",
        "            return nn.Sequential(*net_layers)\n",
        "        else:\n",
        "            return nn.Sequential(nn.Linear(num_features, num_targets), nn.Sigmoid())\n",
        "\n",
        "    # runs the given net on instances of a given size\n",
        "    def eval_opt(net, instances, size):\n",
        "        net.eval()\n",
        "        val = 0.\n",
        "        for i in range(len(instances)):\n",
        "            pred = net(data[size][i])\n",
        "            x = ContinuousOptimizer.apply(pred)\n",
        "            pp, _ = f_true[size][i] #audrey fix\n",
        "            val += objective_coverage(x.detach().numpy().round(), pp.detach().numpy()) #audrey fix\n",
        "        net.train()\n",
        "        return val/len(instances)\n",
        "\n",
        "    # train a network for each size, and test on each sizes\n",
        "    for train_idx, train_size in enumerate(instance_sizes):\n",
        "        net = make_fc()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "        # training\n",
        "        for t in range(num_iters):\n",
        "            print(f\"Iteration {t}\")\n",
        "            ### print(\"Get the model predictions...\") ###\n",
        "            i = random.randint(0, 80)\n",
        "            y = data[train_size][0]\n",
        "            pred = net(y)\n",
        "            ### print(\"Get the optimal solution to the continous problem...\")\n",
        "            x = ContinuousOptimizer.apply(pred)\n",
        "            ### print(\"Get the objective value and set as the loss...\")\n",
        "            pp, _ = f_true[train_size][0]\n",
        "            loss = -CoverageInstanceMultilinear.apply(x, pp)\n",
        "            print(\"training loss\", loss)\n",
        "            ### print(\"Update model weights based on the computed gradients...\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            #break\n",
        "        #break\n",
        "        # save learned network state\n",
        "        savepath = '/tmp/net_diffopt_smalllr_{0}_{1}_{2}_{3}.pt'.format(  #audrey fixes /tmp/\n",
        "            train_size, 2, num_layers, idx)\n",
        "        torch.save(net.state_dict(), savepath)\n",
        "        # test on different sizes\n",
        "        for test_idx, test_size in enumerate(instance_sizes):\n",
        "            vals[idx, train_idx, test_idx] = eval_opt(\n",
        "                net, test, test_size)\n",
        "            print(vals[idx, train_idx, test_idx])\n",
        "        # save out values\n",
        "        print(idx, train_size, vals[idx, train_idx])\n",
        "        with open('results_recommendation_' + str(num_layers) + '.pickle', 'wb') as f:\n",
        "            pickle.dump(vals, f)\n",
        "    #break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYHY5R4vwQnx",
        "outputId": "e2e69984-4977-4b67-cf7c-22766013c600"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 1\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 2\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 3\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 4\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 5\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 6\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 7\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 8\n",
            "training loss tensor(-3.8139, grad_fn=<NegBackward0>)\n",
            "Iteration 9\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 10\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 11\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 12\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 13\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 14\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 15\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 16\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 17\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 18\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 19\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 20\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 21\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 22\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 23\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 24\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 25\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 26\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 27\n",
            "training loss tensor(-3.8140, grad_fn=<NegBackward0>)\n",
            "Iteration 28\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 29\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 30\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 31\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 32\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 33\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 34\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 35\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 36\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 37\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 38\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 39\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 40\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 41\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 42\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 43\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 44\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 45\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 46\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 47\n",
            "training loss tensor(-3.8141, grad_fn=<NegBackward0>)\n",
            "Iteration 48\n",
            "training loss tensor(-3.8142, grad_fn=<NegBackward0>)\n",
            "Iteration 49\n",
            "training loss tensor(-3.8142, grad_fn=<NegBackward0>)\n",
            "3.81416479870677\n",
            "0 0 [3.8141648]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.round()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh_a4W60dBol",
        "outputId": "eb411ad7-87a3-4492-ff65-ec29998d2b81"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0.], grad_fn=<RoundBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06BaTtuydibu",
        "outputId": "121550ac-f39c-44e5-88e5-a1f5af93a43c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8470, 0.5541, 0.0259, 0.8340],\n",
              "        [0.7761, 0.5115, 0.0495, 0.7816],\n",
              "        [0.9792, 0.7313, 0.0012, 0.9594],\n",
              "        [0.8289, 0.5420, 0.0312, 0.8201],\n",
              "        [0.6700, 0.4625, 0.1007, 0.7084],\n",
              "        [0.7520, 0.4992, 0.0593, 0.7646],\n",
              "        [0.6989, 0.4747, 0.0846, 0.7280],\n",
              "        [0.9043, 0.6019, 0.0122, 0.8809],\n",
              "        [0.8921, 0.5901, 0.0147, 0.8704],\n",
              "        [0.7263, 0.4870, 0.0709, 0.7467],\n",
              "        [0.7985, 0.5237, 0.0412, 0.7977],\n",
              "        [0.9452, 0.6534, 0.0052, 0.9196],\n",
              "        [0.5099, 0.4025, 0.2261, 0.5995],\n",
              "        [0.8786, 0.5782, 0.0178, 0.8591],\n",
              "        [0.8192, 0.5359, 0.0342, 0.8129],\n",
              "        [0.8921, 0.5901, 0.0147, 0.8704],\n",
              "        [0.8712, 0.5722, 0.0196, 0.8531],\n",
              "        [0.8091, 0.5298, 0.0376, 0.8054],\n",
              "        [0.8192, 0.5359, 0.0342, 0.8129],\n",
              "        [0.6989, 0.4747, 0.0846, 0.7280],\n",
              "        [0.5099, 0.4025, 0.2261, 0.5995],\n",
              "        [0.9043, 0.6019, 0.0122, 0.8809],\n",
              "        [0.8712, 0.5722, 0.0196, 0.8531],\n",
              "        [0.9251, 0.6251, 0.0083, 0.8998],\n",
              "        [0.3553, 0.3480, 0.4207, 0.4859],\n",
              "        [0.8636, 0.5662, 0.0215, 0.8469]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## recommendation_nn_decision.py\n",
        "## synthetic problem\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# load probability matrix \n",
        "P_list = [\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "]\n",
        "\n",
        "# load features\n",
        "circuit_km = [\n",
        "3.7, \n",
        "3, \n",
        "6.9, \n",
        "3.5, \n",
        "2.2, \n",
        "2.8, \n",
        "2.4, \n",
        "4.5, \n",
        "4.3, \n",
        "2.6, \n",
        "3.2, \n",
        "5.4, \n",
        "1.2, \n",
        "4.1, \n",
        "3.4, \n",
        "4.3, \n",
        "4, \n",
        "3.3, \n",
        "3.4, \n",
        "2.4, \n",
        "1.2, \n",
        "4.5, \n",
        "4, \n",
        "4.9, \n",
        "0.25, \n",
        "3.9, \n",
        "]\n",
        "y = []\n",
        "for i in circuit_km:\n",
        "  y.append([i])\n",
        "\n",
        "num_layers = 1\n",
        "activation = 'relu'\n",
        "#k = 2\n",
        "#use_hessian = False\n",
        "num_iters = 50\n",
        "instance_sizes = [0]\n",
        "learning_rate = 1e-4\n",
        "\n",
        "Ps = {}\n",
        "data = {}\n",
        "f_true = {}\n",
        "\n",
        "for num_items in instance_sizes:\n",
        "    Ps_size = np.array(P_list)\n",
        "    data_size = np.array(y)\n",
        "\n",
        "    num_targets = Ps_size.shape[1] #500 --> 4\n",
        "    num_features = data_size.shape[1] #2113 --> 1\n",
        "    Ps[num_items] = [torch.from_numpy(Ps_size).long()]\n",
        "    data[num_items] = [torch.from_numpy(data_size).float()]\n",
        "    w = np.ones(num_targets, dtype=np.float32)\n",
        "    f_true[num_items] = [(P, w) for P in Ps[num_items]]\n",
        "  \n",
        "num_repetitions = 0\n",
        "\n",
        "train = {}\n",
        "test = {}\n",
        "for size in instance_sizes:\n",
        "  train[size], test[size] = np.array(P_list), np.array(y)"
      ],
      "metadata": {
        "id": "ehZESn1UV3vk"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}