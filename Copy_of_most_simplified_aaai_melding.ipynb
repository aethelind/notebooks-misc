{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOi40ufencuJQd4/MNIhivy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aethelind/notebooks-misc/blob/main/Copy_of_most_simplified_aaai_melding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## submodular.py"
      ],
      "metadata": {
        "id": "odwsZHjsYcW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YvmZ_k9WwJpz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.sparse\n",
        "import scipy.linalg\n",
        "\n",
        "\n",
        "class ContinuousOptimizer(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    pytorch module for differentiable submodular maximization. The forward pass \n",
        "    computes the optimal x for given parameters. The backward pass differentiates \n",
        "    that optimal x wrt the parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, params):\n",
        "        \"\"\"\n",
        "        Computes the optimal x using the supplied optimizer. \n",
        "        \"\"\"\n",
        "        with torch.enable_grad():\n",
        "            x = optimize_coverage_multilinear(P=params, verbose=True, k=20, c=0.95, minibatch_size=None)\n",
        "        ctx.save_for_backward(params, x) \n",
        "        return x.data\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Differentiates the optimal x returned by the forward pass with respect\n",
        "        to the ratings matrix that was given as input.\n",
        "        \"\"\"\n",
        "        params, x = ctx.saved_tensors \n",
        "        xgrad = x.grad.data\n",
        "        dxdr = ContinuousOptimizer.get_dxdr(x.data.detach().numpy(), -xgrad.detach().numpy(), params.detach().numpy(), dgrad_coverage, 0.95)\n",
        "        dxdr_t = torch.from_numpy(np.transpose(dxdr))\n",
        "        out = torch.mm(dxdr_t.float(), grad_output.view(len(x), 1))\n",
        "        return out.view_as(params)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dxdr(x, grad, params, get_dgradf_dparams, max_x):\n",
        "        '''\n",
        "        Returns the derivative of the optimal solution in the region around x in \n",
        "        terms of the rating matrix r. \n",
        "\n",
        "        x: an optimal solution\n",
        "\n",
        "        grad: df/dx at x\n",
        "\n",
        "        params: the current parameter settings\n",
        "        '''\n",
        "        n = len(x)\n",
        "        # first get the optimal dual variables via the KKT conditions\n",
        "        # dual variable for constraint sum(x) <= k\n",
        "        if np.logical_and(x > 0, x < max_x).any():\n",
        "            lambda_sum = np.mean(grad[np.logical_and(x > 0, x < max_x)])\n",
        "        else:\n",
        "            lambda_sum = 0\n",
        "        # dual variable for constraint x <= max_x\n",
        "        lambda_upper = []\n",
        "        # dual variable for constraint x >= 0\n",
        "        lambda_lower = []\n",
        "        for i in range(n):\n",
        "            if np.abs(x[i] - max_x) < 0.000001:\n",
        "                lambda_upper.append(grad[i] - lambda_sum)\n",
        "            else:\n",
        "                lambda_upper.append(0)\n",
        "            if x[i] > 0:\n",
        "                lambda_lower.append(0)\n",
        "            else:\n",
        "                lambda_lower.append(grad[i] - lambda_sum)\n",
        "        # number of constraints\n",
        "        m = 2*n + 1\n",
        "        # collect value of dual variables\n",
        "        lam = np.zeros((m))\n",
        "        lam[0] = lambda_sum\n",
        "        lam[1:(n+1)] = lambda_upper\n",
        "        lam[n+1:] = lambda_lower\n",
        "        diag_lambda = np.matrix(np.diag(lam))\n",
        "        # collect value of constraints\n",
        "        g = np.zeros((m))\n",
        "        # TODO: replace the second x.sum() with k so that this is actually generally correct\n",
        "        g[0] = x.sum() - x.sum()\n",
        "        g[1:(n+1)] = x - max_x\n",
        "        g[n+1:] = -x\n",
        "        diag_g = np.matrix(np.diag(g))\n",
        "        # gradient of constraints wrt x\n",
        "        dgdx = np.zeros((m, n))\n",
        "        # gradient of constraint sum(x) <= k\n",
        "        dgdx[0, :] = 1\n",
        "        # gradient of constraints x <= 1\n",
        "        for i in range(1, n+1):\n",
        "            dgdx[i, i-1] = 1\n",
        "        # gradient of constraints x >= 0 <--> -x <= 0\n",
        "        for i in range(n+1, m):\n",
        "            dgdx[i, i-(n+1)] = -1\n",
        "        dgdx = np.matrix(dgdx)\n",
        "        # the Hessian matrix -- all zeros for now\n",
        "        H = np.matrix(np.zeros((n, n)))\n",
        "        # coefficient matrix for the linear system\n",
        "        A = np.bmat([[H, np.transpose(dgdx)], [diag_lambda*dgdx, diag_g]])\n",
        "        # add 0.01*I to improve conditioning\n",
        "        A = A + 0.01*np.eye(n+m)\n",
        "        # RHS of the linear system, mostly partial derivative of grad f wrt params\n",
        "        dgradf_dparams = get_dgradf_dparams(x, params)\n",
        "        reshaped = np.zeros(\n",
        "            (dgradf_dparams.shape[0], dgradf_dparams.shape[1]*dgradf_dparams.shape[2]))\n",
        "        for i in range(n):\n",
        "            reshaped[i] = dgradf_dparams[i].flatten()\n",
        "        b = np.bmat([[reshaped], [np.zeros((m, reshaped.shape[1]))]])\n",
        "        # solution to the system\n",
        "        derivatives = sp.linalg.solve(A, b)\n",
        "        if np.isnan(derivatives).any():\n",
        "            print('report')\n",
        "            print(np.isnan(A).any())\n",
        "            print(np.isnan(b).any())\n",
        "            print(np.isnan(dgdx).any())\n",
        "            print(np.isnan(diag_lambda).any())\n",
        "            print(np.isnan(diag_g).any())\n",
        "            print(np.isnan(dgradf_dparams).any())\n",
        "        # first n are derivatives of primal variables\n",
        "        derivatives = derivatives[:n]\n",
        "        return derivatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coverage.py"
      ],
      "metadata": {
        "id": "N8c85RN6YfLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "\n",
        "\n",
        "@jit\n",
        "def gradient_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    m = len(x)\n",
        "    grad = np.zeros(m, dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:, i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        for j in range(m):\n",
        "            grad[j] += P[j, i] * p_all_fail/p_fail[j]\n",
        "    return grad\n",
        "\n",
        "\n",
        "@jit\n",
        "def objective_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    total = 0\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:, i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        total += (1 - p_all_fail)\n",
        "    return total\n",
        "\n",
        "\n",
        "class CoverageInstanceMultilinear(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Represents a coverage instance with given coverage probabilities\n",
        "    P. Forward pass computes the objective value (if evaluate_forward\n",
        "    is true). Backward computes the gradients w.r.t. decision variables x.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, P):\n",
        "        ctx.save_for_backward(x, P)\n",
        "        out = objective_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        return torch.tensor(out).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        x, P = ctx.saved_tensors\n",
        "        grad = gradient_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        return torch.from_numpy(grad).float()*grad_in.float(), None\n",
        "\n",
        "\n",
        "def optimize_coverage_multilinear(P, verbose=True, k=10, c=1., minibatch_size=None):\n",
        "    '''\n",
        "    Run some variant of SGD for the coverage problem with given \n",
        "    coverage probabilities P.\n",
        "    '''\n",
        "    # decision variables\n",
        "    x = torch.zeros(P.shape[0], requires_grad=True)\n",
        "    # set up the optimizer\n",
        "    optimizer = torch.optim.SGD([x], momentum=0.9, lr=0.1, nesterov=True)\n",
        "    # take projected stochastic gradient steps\n",
        "    for t in range(20):\n",
        "        # objective which will provide gradient evaluations\n",
        "        loss = -CoverageInstanceMultilinear.apply(x, P)\n",
        "        #if verbose:\n",
        "            #print(t, -loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        x.data = torch.from_numpy(project_uniform_matroid_boundary(x.data.numpy(), k, 1/c)).float()\n",
        "    return x\n",
        "\n",
        "\n",
        "@jit\n",
        "def dgrad_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    m = len(x)\n",
        "    dgrad = np.zeros((m, m, n), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:,i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        for j in range(m):\n",
        "            for k in range(m):\n",
        "                if j == k:\n",
        "                    dgrad[j, k, i] = p_all_fail/p_fail[j]\n",
        "                else:\n",
        "                    dgrad[j, k, i] = -x[k] * P[j, i] * p_all_fail/(p_fail[j] * p_fail[k])\n",
        "    return dgrad"
      ],
      "metadata": {
        "id": "SrYMOsSPwM5p"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "xWd82ffpYkF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_uniform_matroid_boundary(x, k, c=1):\n",
        "    '''\n",
        "    Exact projection algorithm of Karimi et al. This is the projection implementation\n",
        "    that should be used now.\n",
        "    \n",
        "    Projects x onto the set {y: 0 <= y <= 1/c, ||y||_1 = k}\n",
        "    '''\n",
        "    import numpy as np\n",
        "    k *= c\n",
        "    n = len(x)\n",
        "    x = x.copy()\n",
        "    alpha_upper = x/c\n",
        "    alpha_lower = (x*c - 1)/c**2\n",
        "    S = []\n",
        "    S.extend(alpha_lower)\n",
        "    S.extend(alpha_upper)\n",
        "    S.sort()\n",
        "    S = np.unique(S)\n",
        "    h = n\n",
        "    alpha = min(S) - 1\n",
        "    m = 0\n",
        "    for i in range(len(S)):\n",
        "        hprime = h + (S[i] - alpha)*m\n",
        "        if hprime < k and k <= h:\n",
        "            alphastar = (S[i] - alpha)*(h - k)/(h - hprime) + alpha\n",
        "            result = np.zeros((n))\n",
        "            for j in range(n):\n",
        "                if alpha_lower[j] > alphastar:\n",
        "                    result[j] = 1./c\n",
        "                elif alpha_upper[j] >= alphastar:\n",
        "                    result[j] = x[j] - alphastar*c\n",
        "            return result\n",
        "        m -= (alpha_lower == S[i]).sum()*(c**2)\n",
        "        m += (alpha_upper == S[i]).sum()*(c**2)\n",
        "        h = hprime\n",
        "        alpha = S[i]\n",
        "    raise Exception('projection did not terminate')"
      ],
      "metadata": {
        "id": "cl6N0_01wOyP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear out directory\n",
        "!rm -rf *\n",
        "# Download data_decisions_benchmarks.zip and unzip diverse_recommendation_data.pickle\n",
        "!curl https://bryanwilder.github.io/files/data_decisions_benchmarks.zip | jar xv benchmarks_release/diverse_recommendation_data.pickle\n",
        "# Move diverse_recommendation_data.pickle to current directory\n",
        "!mv benchmarks_release/diverse_recommendation_data.pickle .\n",
        "# Remove empty directory\n",
        "!rm -rf benchmarks_release\n",
        "# Download hetrec2011-movielens-2k-v2.zip and unzip movie_actors.dat and user_ratedmovies.dat\n",
        "!curl https://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip | jar xv movie_actors.dat user_ratedmovies.dat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GstNrrAwT_Y",
        "outputId": "7edf9f98-16e1-4b03-9082-f682d1ad2f24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 83.0M  100 83.0M    0     0  22.6M      0  0:00:03  0:00:03 --:--:-- 22.6M\n",
            " inflated: benchmarks_release/diverse_recommendation_data.pickle\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            " 14 17.9M   14 2736k    0     0  6772k      0  0:00:02 --:--:--  0:00:02 6755k inflated: movie_actors.dat\n",
            " inflated: user_ratedmovies.dat\n",
            "100 17.9M  100 17.9M    0     0  18.1M      0 --:--:-- --:--:-- --:--:-- 18.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## recommendation_nn_decision.py\n",
        "## movie problem\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "num_layers = 1\n",
        "activation = 'relu'\n",
        "#k = 20\n",
        "#use_hessian = False\n",
        "num_iters = 100\n",
        "instance_sizes = [100]\n",
        "learning_rate = 1e-4\n",
        "\n",
        "Ps = {}\n",
        "data = {}\n",
        "f_true = {}\n",
        "for num_items in instance_sizes:\n",
        "    with open('diverse_recommendation_data' + '.pickle', 'rb') as f:\n",
        "        # Ps_size takes on the actors 100x500 and data_size takes the users 100x2113\n",
        "        Ps_size, data_size = pickle.load(f)\n",
        "\n",
        "    num_targets = Ps_size[0].shape[1]\n",
        "    num_features = data_size[0].shape[1]\n",
        "    Ps[num_items] = [torch.from_numpy(P).long() for P in Ps_size]\n",
        "    data[num_items] = [torch.from_numpy(x).float() for x in data_size]\n",
        "    w = np.ones(num_targets, dtype=np.float32)\n",
        "    f_true[num_items] = [(P, w) for P in Ps[num_items]]\n",
        "\n",
        "\n",
        "num_repetitions = 0\n",
        "\n",
        "train = {}\n",
        "test = {}\n",
        "for size in instance_sizes:\n",
        "    with open('diverse_recommendation_data' + '.pickle', 'rb') as f:\n",
        "        train[size], test[size] = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "uBQ7Tb0GV0TA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ps_size[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T0HV5fcU-91",
        "outputId": "21cc2618-4bb8-48be-a4ea-7bc4c54c5b77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ps_size[0][0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3dPxSBVVgQH",
        "outputId": "145e6793-4e3a-400c-d726-9405f9ea663f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_size[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMKPXbclVKbZ",
        "outputId": "b3e0a29f-0912-4f60-f0c5-b443a66ddf5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2113)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "mrcmbbiPYXeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals = np.zeros((num_repetitions+30, len(instance_sizes), len(instance_sizes)))\n",
        "\n",
        "for idx in range(num_repetitions, num_repetitions+1): #+ 30):\n",
        "\n",
        "    intermediate_size = 200\n",
        "    def make_fc():\n",
        "        if num_layers > 1:\n",
        "            if activation == 'relu':\n",
        "                activation_fn = nn.ReLU\n",
        "            elif activation == 'sigmoid':\n",
        "                activation_fn = nn.Sigmoid\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    'Invalid activation function: ' + str(activation))\n",
        "            net_layers = [\n",
        "                nn.Linear(num_features, intermediate_size), activation_fn()]\n",
        "            for hidden in range(num_layers-2):\n",
        "                net_layers.append(\n",
        "                    nn.Linear(intermediate_size, intermediate_size))\n",
        "                net_layers.append(activation_fn())\n",
        "            net_layers.append(nn.Linear(intermediate_size, num_targets))\n",
        "            net_layers.append(nn.Sigmoid())\n",
        "            return nn.Sequential(*net_layers)\n",
        "        else:\n",
        "            return nn.Sequential(nn.Linear(num_features, num_targets), nn.Sigmoid())\n",
        "\n",
        "    # runs the given net on instances of a given size\n",
        "    def eval_opt(net, instances, size):\n",
        "        net.eval()\n",
        "        val = 0.\n",
        "        for i in range(len(instances)):\n",
        "            pred = net(data[size][i])\n",
        "            x = ContinuousOptimizer.apply(pred)\n",
        "            pp, _ = f_true[size][i] #audrey fix\n",
        "            val += objective_coverage(x.detach().numpy().round(), pp.detach().numpy()) #audrey fix\n",
        "        net.train()\n",
        "        return val/len(instances)\n",
        "\n",
        "    # train a network for each size, and test on each sizes\n",
        "    for train_idx, train_size in enumerate(instance_sizes):\n",
        "        net = make_fc()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "        # training\n",
        "        for t in range(num_iters):\n",
        "            print(f\"Iteration {t}\")\n",
        "            ### print(\"Get the model predictions...\") ###\n",
        "            i = random.randint(0, 80)\n",
        "            y = data[train_size][i]\n",
        "            pred = net(y)\n",
        "            ### print(\"Get the optimal solution to the continous problem...\")\n",
        "            x = ContinuousOptimizer.apply(pred)\n",
        "            ### print(\"Get the objective value and set as the loss...\")\n",
        "            pp, _ = f_true[train_size][i]\n",
        "            loss = -CoverageInstanceMultilinear.apply(x, pp)\n",
        "            print(\"training loss\", loss)\n",
        "            ### print(\"Update model weights based on the computed gradients...\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            #break\n",
        "        #break\n",
        "        # save learned network state\n",
        "        savepath = '/tmp/net_diffopt_smalllr_{0}_{1}_{2}_{3}.pt'.format(  #audrey fixes /tmp/\n",
        "            train_size, 2, num_layers, idx)\n",
        "        torch.save(net.state_dict(), savepath)\n",
        "        # test on different sizes\n",
        "        for test_idx, test_size in enumerate(instance_sizes):\n",
        "            vals[idx, train_idx, test_idx] = eval_opt(\n",
        "                net, test, test_size)\n",
        "            print(vals[idx, train_idx, test_idx])\n",
        "        # save out values\n",
        "        print(idx, train_size, vals[idx, train_idx])\n",
        "        with open('results_recommendation_' + str(num_layers) + '.pickle', 'wb') as f:\n",
        "            pickle.dump(vals, f)\n",
        "    #break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYHY5R4vwQnx",
        "outputId": "3080f728-ead3-45e2-eff7-f98ec45d3979"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "training loss tensor(-17.0664, grad_fn=<NegBackward0>)\n",
            "Iteration 1\n",
            "training loss tensor(-45.8804, grad_fn=<NegBackward0>)\n",
            "Iteration 2\n",
            "training loss tensor(-51.6705, grad_fn=<NegBackward0>)\n",
            "Iteration 3\n",
            "training loss tensor(-60.3399, grad_fn=<NegBackward0>)\n",
            "Iteration 4\n",
            "training loss tensor(-46.5997, grad_fn=<NegBackward0>)\n",
            "Iteration 5\n",
            "training loss tensor(-23.5520, grad_fn=<NegBackward0>)\n",
            "Iteration 6\n",
            "training loss tensor(-53.4883, grad_fn=<NegBackward0>)\n",
            "Iteration 7\n",
            "training loss tensor(-29.5868, grad_fn=<NegBackward0>)\n",
            "Iteration 8\n",
            "training loss tensor(-48.5714, grad_fn=<NegBackward0>)\n",
            "Iteration 9\n",
            "training loss tensor(-54.6399, grad_fn=<NegBackward0>)\n",
            "Iteration 10\n",
            "training loss tensor(-43.0479, grad_fn=<NegBackward0>)\n",
            "Iteration 11\n",
            "training loss tensor(-29.0924, grad_fn=<NegBackward0>)\n",
            "Iteration 12\n",
            "training loss tensor(-51.0705, grad_fn=<NegBackward0>)\n",
            "Iteration 13\n",
            "training loss tensor(-56.6278, grad_fn=<NegBackward0>)\n",
            "Iteration 14\n",
            "training loss tensor(-69.7474, grad_fn=<NegBackward0>)\n",
            "Iteration 15\n",
            "training loss tensor(-32.1089, grad_fn=<NegBackward0>)\n",
            "Iteration 16\n",
            "training loss tensor(-58.8935, grad_fn=<NegBackward0>)\n",
            "Iteration 17\n",
            "training loss tensor(-36.1366, grad_fn=<NegBackward0>)\n",
            "Iteration 18\n",
            "training loss tensor(-36.8260, grad_fn=<NegBackward0>)\n",
            "Iteration 19\n",
            "training loss tensor(-47.3867, grad_fn=<NegBackward0>)\n",
            "Iteration 20\n",
            "training loss tensor(-43.8852, grad_fn=<NegBackward0>)\n",
            "Iteration 21\n",
            "training loss tensor(-92.6072, grad_fn=<NegBackward0>)\n",
            "Iteration 22\n",
            "training loss tensor(-48.6271, grad_fn=<NegBackward0>)\n",
            "Iteration 23\n",
            "training loss tensor(-48.5093, grad_fn=<NegBackward0>)\n",
            "Iteration 24\n",
            "training loss tensor(-53.4142, grad_fn=<NegBackward0>)\n",
            "Iteration 25\n",
            "training loss tensor(-51.3054, grad_fn=<NegBackward0>)\n",
            "Iteration 26\n",
            "training loss tensor(-45.4952, grad_fn=<NegBackward0>)\n",
            "Iteration 27\n",
            "training loss tensor(-54.7087, grad_fn=<NegBackward0>)\n",
            "Iteration 28\n",
            "training loss tensor(-50.4694, grad_fn=<NegBackward0>)\n",
            "Iteration 29\n",
            "training loss tensor(-46.4032, grad_fn=<NegBackward0>)\n",
            "Iteration 30\n",
            "training loss tensor(-73.1029, grad_fn=<NegBackward0>)\n",
            "Iteration 31\n",
            "training loss tensor(-31.6347, grad_fn=<NegBackward0>)\n",
            "Iteration 32\n",
            "training loss tensor(-42.5737, grad_fn=<NegBackward0>)\n",
            "Iteration 33\n",
            "training loss tensor(-74.7535, grad_fn=<NegBackward0>)\n",
            "Iteration 34\n",
            "training loss tensor(-54.2333, grad_fn=<NegBackward0>)\n",
            "Iteration 35\n",
            "training loss tensor(-82.7529, grad_fn=<NegBackward0>)\n",
            "Iteration 36\n",
            "training loss tensor(-54.7750, grad_fn=<NegBackward0>)\n",
            "Iteration 37\n",
            "training loss tensor(-56.0295, grad_fn=<NegBackward0>)\n",
            "Iteration 38\n",
            "training loss tensor(-57.7738, grad_fn=<NegBackward0>)\n",
            "Iteration 39\n",
            "training loss tensor(-45.0771, grad_fn=<NegBackward0>)\n",
            "Iteration 40\n",
            "training loss tensor(-46.3968, grad_fn=<NegBackward0>)\n",
            "Iteration 41\n",
            "training loss tensor(-27.1783, grad_fn=<NegBackward0>)\n",
            "Iteration 42\n",
            "training loss tensor(-66.9375, grad_fn=<NegBackward0>)\n",
            "Iteration 43\n",
            "training loss tensor(-46.6395, grad_fn=<NegBackward0>)\n",
            "Iteration 44\n",
            "training loss tensor(-47.1104, grad_fn=<NegBackward0>)\n",
            "Iteration 45\n",
            "training loss tensor(-66.9375, grad_fn=<NegBackward0>)\n",
            "Iteration 46\n",
            "training loss tensor(-48.6400, grad_fn=<NegBackward0>)\n",
            "Iteration 47\n",
            "training loss tensor(-53.9645, grad_fn=<NegBackward0>)\n",
            "Iteration 48\n",
            "training loss tensor(-32.0541, grad_fn=<NegBackward0>)\n",
            "Iteration 49\n",
            "training loss tensor(-55.2425, grad_fn=<NegBackward0>)\n",
            "Iteration 50\n",
            "training loss tensor(-38.3727, grad_fn=<NegBackward0>)\n",
            "Iteration 51\n",
            "training loss tensor(-67.6122, grad_fn=<NegBackward0>)\n",
            "Iteration 52\n",
            "training loss tensor(-75.3061, grad_fn=<NegBackward0>)\n",
            "Iteration 53\n",
            "training loss tensor(-78.6033, grad_fn=<NegBackward0>)\n",
            "Iteration 54\n",
            "training loss tensor(-54.4686, grad_fn=<NegBackward0>)\n",
            "Iteration 55\n",
            "training loss tensor(-53.4425, grad_fn=<NegBackward0>)\n",
            "Iteration 56\n",
            "training loss tensor(-42.9355, grad_fn=<NegBackward0>)\n",
            "Iteration 57\n",
            "training loss tensor(-41.5274, grad_fn=<NegBackward0>)\n",
            "Iteration 58\n",
            "training loss tensor(-48.3106, grad_fn=<NegBackward0>)\n",
            "Iteration 59\n",
            "training loss tensor(-45.6214, grad_fn=<NegBackward0>)\n",
            "Iteration 60\n",
            "training loss tensor(-48.6400, grad_fn=<NegBackward0>)\n",
            "Iteration 61\n",
            "training loss tensor(-52.4475, grad_fn=<NegBackward0>)\n",
            "Iteration 62\n",
            "training loss tensor(-51.6799, grad_fn=<NegBackward0>)\n",
            "Iteration 63\n",
            "training loss tensor(-46.4175, grad_fn=<NegBackward0>)\n",
            "Iteration 64\n",
            "training loss tensor(-45.3856, grad_fn=<NegBackward0>)\n",
            "Iteration 65\n",
            "training loss tensor(-75.8260, grad_fn=<NegBackward0>)\n",
            "Iteration 66\n",
            "training loss tensor(-54.9427, grad_fn=<NegBackward0>)\n",
            "Iteration 67\n",
            "training loss tensor(-28.2712, grad_fn=<NegBackward0>)\n",
            "Iteration 68\n",
            "training loss tensor(-46.0103, grad_fn=<NegBackward0>)\n",
            "Iteration 69\n",
            "training loss tensor(-48.6400, grad_fn=<NegBackward0>)\n",
            "Iteration 70\n",
            "training loss tensor(-38.1937, grad_fn=<NegBackward0>)\n",
            "Iteration 71\n",
            "training loss tensor(-82.3883, grad_fn=<NegBackward0>)\n",
            "Iteration 72\n",
            "training loss tensor(-28.6862, grad_fn=<NegBackward0>)\n",
            "Iteration 73\n",
            "training loss tensor(-64.8947, grad_fn=<NegBackward0>)\n",
            "Iteration 74\n",
            "training loss tensor(-26.3917, grad_fn=<NegBackward0>)\n",
            "Iteration 75\n",
            "training loss tensor(-37.2775, grad_fn=<NegBackward0>)\n",
            "Iteration 76\n",
            "training loss tensor(-54.8708, grad_fn=<NegBackward0>)\n",
            "Iteration 77\n",
            "training loss tensor(-51.1645, grad_fn=<NegBackward0>)\n",
            "Iteration 78\n",
            "training loss tensor(-43.0980, grad_fn=<NegBackward0>)\n",
            "Iteration 79\n",
            "training loss tensor(-62.1143, grad_fn=<NegBackward0>)\n",
            "Iteration 80\n",
            "training loss tensor(-55.1281, grad_fn=<NegBackward0>)\n",
            "Iteration 81\n",
            "training loss tensor(-52.4450, grad_fn=<NegBackward0>)\n",
            "Iteration 82\n",
            "training loss tensor(-55.2425, grad_fn=<NegBackward0>)\n",
            "Iteration 83\n",
            "training loss tensor(-66.0669, grad_fn=<NegBackward0>)\n",
            "Iteration 84\n",
            "training loss tensor(-48.9753, grad_fn=<NegBackward0>)\n",
            "Iteration 85\n",
            "training loss tensor(-51.8971, grad_fn=<NegBackward0>)\n",
            "Iteration 86\n",
            "training loss tensor(-28.4441, grad_fn=<NegBackward0>)\n",
            "Iteration 87\n",
            "training loss tensor(-52.5438, grad_fn=<NegBackward0>)\n",
            "Iteration 88\n",
            "training loss tensor(-28.8843, grad_fn=<NegBackward0>)\n",
            "Iteration 89\n",
            "training loss tensor(-52.0646, grad_fn=<NegBackward0>)\n",
            "Iteration 90\n",
            "training loss tensor(-57.9825, grad_fn=<NegBackward0>)\n",
            "Iteration 91\n",
            "training loss tensor(-50.6745, grad_fn=<NegBackward0>)\n",
            "Iteration 92\n",
            "training loss tensor(-34.2475, grad_fn=<NegBackward0>)\n",
            "Iteration 93\n",
            "training loss tensor(-50.9030, grad_fn=<NegBackward0>)\n",
            "Iteration 94\n",
            "training loss tensor(-66.7349, grad_fn=<NegBackward0>)\n",
            "Iteration 95\n",
            "training loss tensor(-60.3032, grad_fn=<NegBackward0>)\n",
            "Iteration 96\n",
            "training loss tensor(-62.5590, grad_fn=<NegBackward0>)\n",
            "Iteration 97\n",
            "training loss tensor(-48.0870, grad_fn=<NegBackward0>)\n",
            "Iteration 98\n",
            "training loss tensor(-53.0120, grad_fn=<NegBackward0>)\n",
            "Iteration 99\n",
            "training loss tensor(-60.3976, grad_fn=<NegBackward0>)\n",
            "69.0\n",
            "0 100 [69.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.round()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh_a4W60dBol",
        "outputId": "493e524d-e965-4f48-9ebc-90c0e5b0d6c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
              "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], grad_fn=<RoundBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06BaTtuydibu",
        "outputId": "54f88e4d-8a05-428c-9182-4bce92b28ecd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6899, 0.9460, 0.9289,  ..., 0.7512, 0.7547, 0.5881],\n",
              "        [0.7172, 0.3402, 0.3255,  ..., 0.5605, 0.7332, 0.8300],\n",
              "        [0.6713, 0.4358, 0.6057,  ..., 0.6982, 0.6583, 0.3758],\n",
              "        ...,\n",
              "        [0.5496, 0.4726, 0.4089,  ..., 0.5988, 0.4816, 0.5596],\n",
              "        [0.5360, 0.4260, 0.4028,  ..., 0.4339, 0.4665, 0.4471],\n",
              "        [0.5491, 0.4958, 0.4940,  ..., 0.5498, 0.4901, 0.4619]],\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## recommendation_nn_decision.py\n",
        "## synthetic problem\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# load probability matrix \n",
        "P_list = [\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "]\n",
        "\n",
        "# load features\n",
        "circuit_km = [\n",
        "3.7, \n",
        "3, \n",
        "6.9, \n",
        "3.5, \n",
        "2.2, \n",
        "2.8, \n",
        "2.4, \n",
        "4.5, \n",
        "4.3, \n",
        "2.6, \n",
        "3.2, \n",
        "5.4, \n",
        "1.2, \n",
        "4.1, \n",
        "3.4, \n",
        "4.3, \n",
        "4, \n",
        "3.3, \n",
        "3.4, \n",
        "2.4, \n",
        "1.2, \n",
        "4.5, \n",
        "4, \n",
        "4.9, \n",
        "0.25, \n",
        "3.9, \n",
        "]\n",
        "y = []\n",
        "for i in circuit_km:\n",
        "  y.append([i])\n",
        "\n",
        "num_layers = 1\n",
        "activation = 'relu'\n",
        "#k = 2\n",
        "#use_hessian = False\n",
        "num_iters = 50\n",
        "instance_sizes = [0]\n",
        "learning_rate = 1e-4\n",
        "\n",
        "Ps = {}\n",
        "data = {}\n",
        "f_true = {}\n",
        "\n",
        "for num_items in instance_sizes:\n",
        "    Ps_size = np.array(P_list)\n",
        "    data_size = np.array(y)\n",
        "\n",
        "    num_targets = Ps_size.shape[1] #500 --> 4\n",
        "    num_features = data_size.shape[1] #2113 --> 1\n",
        "    Ps[num_items] = [torch.from_numpy(Ps_size).long()]\n",
        "    data[num_items] = [torch.from_numpy(data_size).float()]\n",
        "    w = np.ones(num_targets, dtype=np.float32)\n",
        "    f_true[num_items] = [(P, w) for P in Ps[num_items]]\n",
        "  \n",
        "num_repetitions = 0\n",
        "\n",
        "train = {}\n",
        "test = {}\n",
        "for size in instance_sizes:\n",
        "  train[size], test[size] = np.array(P_list), np.array(y)"
      ],
      "metadata": {
        "id": "ehZESn1UV3vk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Optimizer on Ground Truth"
      ],
      "metadata": {
        "id": "I5mZ5GNwavhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_ground = ContinuousOptimizer.apply()"
      ],
      "metadata": {
        "id": "S5_bEQBJafyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}