{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFSRTk6yRUnaAHXcZn7P0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aethelind/notebooks-misc/blob/main/most_simplified_aaai_melding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## submodular.py"
      ],
      "metadata": {
        "id": "odwsZHjsYcW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "YvmZ_k9WwJpz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.sparse\n",
        "import scipy.linalg\n",
        "\n",
        "\n",
        "class ContinuousOptimizer(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    pytorch module for differentiable submodular maximization. The forward pass \n",
        "    computes the optimal x for given parameters. The backward pass differentiates \n",
        "    that optimal x wrt the parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, params):\n",
        "        \"\"\"\n",
        "        Computes the optimal x using the supplied optimizer. \n",
        "        \"\"\"\n",
        "        with torch.enable_grad():\n",
        "            x = optimize_coverage_multilinear(P=params, verbose=True, k=2, c=0.95, minibatch_size=None)\n",
        "        ctx.save_for_backward(params, x) \n",
        "        return x.data\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Differentiates the optimal x returned by the forward pass with respect\n",
        "        to the ratings matrix that was given as input.\n",
        "        \"\"\"\n",
        "        print(\"df(x(theta_hat), theta) / CO grad_output\", grad_output) ###\n",
        "        params, x = ctx.saved_tensors \n",
        "        print(\"theta_hat / params\", params) ###\n",
        "        print(\"x(theta_hat) / x\", x) ###\n",
        "        xgrad = x.grad.data\n",
        "        print(\"dx(theta_hat) / xgrad\", xgrad) ###\n",
        "        dxdr = ContinuousOptimizer.get_dxdr(x.data.detach().numpy(), -xgrad.detach().numpy(), params.detach().numpy(), dgrad_coverage, 0.95)\n",
        "        print(\"dx(theta_hat)/dtheta_hat / dxdr\", dxdr.shape) ###\n",
        "        dxdr_t = torch.from_numpy(np.transpose(dxdr))\n",
        "        print(\"dxdr_t\", dxdr_t.shape) ###\n",
        "        out = torch.mm(dxdr_t.float(), grad_output.view(len(x), 1))\n",
        "        print(\"dfdx*dxdr / CO out\", out.view_as(params))\n",
        "        return out.view_as(params)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dxdr(x, grad, params, get_dgradf_dparams, max_x):\n",
        "        '''\n",
        "        Returns the derivative of the optimal solution in the region around x in \n",
        "        terms of the rating matrix r. \n",
        "\n",
        "        x: an optimal solution\n",
        "\n",
        "        grad: df/dx at x\n",
        "\n",
        "        params: the current parameter settings\n",
        "        '''\n",
        "        n = len(x)\n",
        "        # first get the optimal dual variables via the KKT conditions\n",
        "        # dual variable for constraint sum(x) <= k\n",
        "        if np.logical_and(x > 0, x < max_x).any():\n",
        "            lambda_sum = np.mean(grad[np.logical_and(x > 0, x < max_x)])\n",
        "        else:\n",
        "            lambda_sum = 0\n",
        "        # dual variable for constraint x <= max_x\n",
        "        lambda_upper = []\n",
        "        # dual variable for constraint x >= 0\n",
        "        lambda_lower = []\n",
        "        for i in range(n):\n",
        "            if np.abs(x[i] - max_x) < 0.000001:\n",
        "                lambda_upper.append(grad[i] - lambda_sum)\n",
        "            else:\n",
        "                lambda_upper.append(0)\n",
        "            if x[i] > 0:\n",
        "                lambda_lower.append(0)\n",
        "            else:\n",
        "                lambda_lower.append(grad[i] - lambda_sum)\n",
        "        # number of constraints\n",
        "        m = 2*n + 1\n",
        "        # collect value of dual variables\n",
        "        lam = np.zeros((m))\n",
        "        lam[0] = lambda_sum\n",
        "        lam[1:(n+1)] = lambda_upper\n",
        "        lam[n+1:] = lambda_lower\n",
        "        diag_lambda = np.matrix(np.diag(lam))\n",
        "        # collect value of constraints\n",
        "        g = np.zeros((m))\n",
        "        # TODO: replace the second x.sum() with k so that this is actually generally correct\n",
        "        g[0] = x.sum() - x.sum()\n",
        "        g[1:(n+1)] = x - max_x\n",
        "        g[n+1:] = -x\n",
        "        diag_g = np.matrix(np.diag(g))\n",
        "        # gradient of constraints wrt x\n",
        "        dgdx = np.zeros((m, n))\n",
        "        # gradient of constraint sum(x) <= k\n",
        "        dgdx[0, :] = 1\n",
        "        # gradient of constraints x <= 1\n",
        "        for i in range(1, n+1):\n",
        "            dgdx[i, i-1] = 1\n",
        "        # gradient of constraints x >= 0 <--> -x <= 0\n",
        "        for i in range(n+1, m):\n",
        "            dgdx[i, i-(n+1)] = -1\n",
        "        dgdx = np.matrix(dgdx)\n",
        "        # the Hessian matrix -- all zeros for now\n",
        "        H = np.matrix(np.zeros((n, n)))\n",
        "        # coefficient matrix for the linear system\n",
        "        A = np.bmat([[H, np.transpose(dgdx)], [diag_lambda*dgdx, diag_g]])\n",
        "        # add 0.01*I to improve conditioning\n",
        "        A = A + 0.01*np.eye(n+m)\n",
        "        # RHS of the linear system, mostly partial derivative of grad f wrt params\n",
        "        dgradf_dparams = get_dgradf_dparams(x, params, num_samples=1000)\n",
        "        reshaped = np.zeros(\n",
        "            (dgradf_dparams.shape[0], dgradf_dparams.shape[1]*dgradf_dparams.shape[2]))\n",
        "        for i in range(n):\n",
        "            reshaped[i] = dgradf_dparams[i].flatten()\n",
        "        b = np.bmat([[reshaped], [np.zeros((m, reshaped.shape[1]))]])\n",
        "        # solution to the system\n",
        "        derivatives = sp.linalg.solve(A, b)\n",
        "        if np.isnan(derivatives).any():\n",
        "            print('report')\n",
        "            print(np.isnan(A).any())\n",
        "            print(np.isnan(b).any())\n",
        "            print(np.isnan(dgdx).any())\n",
        "            print(np.isnan(diag_lambda).any())\n",
        "            print(np.isnan(diag_g).any())\n",
        "            print(np.isnan(dgradf_dparams).any())\n",
        "        # first n are derivatives of primal variables\n",
        "        derivatives = derivatives[:n]\n",
        "        return derivatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coverage.py"
      ],
      "metadata": {
        "id": "N8c85RN6YfLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "\n",
        "\n",
        "@jit\n",
        "def gradient_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    m = len(x)\n",
        "    grad = np.zeros(m, dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:, i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        for j in range(m):\n",
        "            grad[j] += P[j, i] * p_all_fail/p_fail[j]\n",
        "    return grad\n",
        "\n",
        "\n",
        "@jit\n",
        "def objective_coverage(x, P):\n",
        "    n = P.shape[1]\n",
        "    total = 0\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:, i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        total += (1 - p_all_fail)\n",
        "    return total\n",
        "\n",
        "\n",
        "class CoverageInstanceMultilinear(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Represents a coverage instance with given coverage probabilities\n",
        "    P. Forward pass computes the objective value (if evaluate_forward\n",
        "    is true). Backward computes the gradients w.r.t. decision variables x.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, P):\n",
        "        ctx.save_for_backward(x, P)\n",
        "        out = objective_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        ### print(\"objective_coverage CoverageInstanceMultilinear forward out\", torch.tensor(out).float()) ###\n",
        "        return torch.tensor(out).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        ## print(\"grad_in\", grad_in) ###\n",
        "        x, P = ctx.saved_tensors\n",
        "        grad = gradient_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        ### print(\"gradient_coverage CoverageInstanceMultilinear backward out\", torch.from_numpy(grad).float()) ###\n",
        "        return torch.from_numpy(grad).float()*grad_in.float(), None\n",
        "\n",
        "\n",
        "def optimize_coverage_multilinear(P, verbose=True, k=10, c=1., minibatch_size=None):\n",
        "    '''\n",
        "    Run some variant of SGD for the coverage problem with given \n",
        "    coverage probabilities P.\n",
        "    '''\n",
        "    # decision variables\n",
        "    x = torch.zeros(P.shape[0], requires_grad=True)\n",
        "    # set up the optimizer\n",
        "    optimizer = torch.optim.SGD([x], momentum=0.9, lr=0.1, nesterov=True)\n",
        "    # take projected stochastic gradient steps\n",
        "    for t in range(10):\n",
        "        # objective which will provide gradient evaluations\n",
        "        loss = -CoverageInstanceMultilinear.apply(x, P)\n",
        "        if verbose:\n",
        "            print(t, -loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        x.data = torch.from_numpy(project_uniform_matroid_boundary(x.data.numpy(), k, 1/c)).float()\n",
        "        ### print(\"x\", x) ###\n",
        "    return x\n",
        "\n",
        "\n",
        "@jit\n",
        "def dgrad_coverage(x, P, num_samples):\n",
        "    n = P.shape[1]\n",
        "    m = len(x)\n",
        "    dgrad = np.zeros((m, m, n), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        p_fail = 1 - x*P[:,i]\n",
        "        p_all_fail = np.prod(p_fail)\n",
        "        for j in range(m):\n",
        "            for k in range(m):\n",
        "                if j == k:\n",
        "                    dgrad[j, k, i] = p_all_fail/p_fail[j]\n",
        "                else:\n",
        "                    dgrad[j, k, i] = -x[k] * P[j, i] * p_all_fail/(p_fail[j] * p_fail[k])\n",
        "    return dgrad"
      ],
      "metadata": {
        "id": "SrYMOsSPwM5p"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "xWd82ffpYkF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_uniform_matroid_boundary(x, k, c=1):\n",
        "    '''\n",
        "    Exact projection algorithm of Karimi et al. This is the projection implementation\n",
        "    that should be used now.\n",
        "    \n",
        "    Projects x onto the set {y: 0 <= y <= 1/c, ||y||_1 = k}\n",
        "    '''\n",
        "    import numpy as np\n",
        "    k *= c\n",
        "    n = len(x)\n",
        "    x = x.copy()\n",
        "    alpha_upper = x/c\n",
        "    alpha_lower = (x*c - 1)/c**2\n",
        "    S = []\n",
        "    S.extend(alpha_lower)\n",
        "    S.extend(alpha_upper)\n",
        "    S.sort()\n",
        "    S = np.unique(S)\n",
        "    h = n\n",
        "    alpha = min(S) - 1\n",
        "    m = 0\n",
        "    for i in range(len(S)):\n",
        "        hprime = h + (S[i] - alpha)*m\n",
        "        if hprime < k and k <= h:\n",
        "            alphastar = (S[i] - alpha)*(h - k)/(h - hprime) + alpha\n",
        "            result = np.zeros((n))\n",
        "            for j in range(n):\n",
        "                if alpha_lower[j] > alphastar:\n",
        "                    result[j] = 1./c\n",
        "                elif alpha_upper[j] >= alphastar:\n",
        "                    result[j] = x[j] - alphastar*c\n",
        "            return result\n",
        "        m -= (alpha_lower == S[i]).sum()*(c**2)\n",
        "        m += (alpha_upper == S[i]).sum()*(c**2)\n",
        "        h = hprime\n",
        "        alpha = S[i]\n",
        "    raise Exception('projection did not terminate')"
      ],
      "metadata": {
        "id": "cl6N0_01wOyP"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear out directory\n",
        "!rm -rf *\n",
        "# Download data_decisions_benchmarks.zip and unzip diverse_recommendation_data.pickle\n",
        "!curl https://bryanwilder.github.io/files/data_decisions_benchmarks.zip | jar xv benchmarks_release/diverse_recommendation_data.pickle\n",
        "# Move diverse_recommendation_data.pickle to current directory\n",
        "!mv benchmarks_release/diverse_recommendation_data.pickle .\n",
        "# Remove empty directory\n",
        "!rm -rf benchmarks_release\n",
        "# Download hetrec2011-movielens-2k-v2.zip and unzip movie_actors.dat and user_ratedmovies.dat\n",
        "!curl https://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip | jar xv movie_actors.dat user_ratedmovies.dat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GstNrrAwT_Y",
        "outputId": "b432164e-ce74-4f52-c0ff-705736ae882d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 83.0M  100 83.0M    0     0  20.8M      0  0:00:03  0:00:03 --:--:-- 20.8M\n",
            " inflated: benchmarks_release/diverse_recommendation_data.pickle\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 inflated: movie_actors.dat\n",
            " 20 17.9M   20 3808k    0     0  6201k      0  0:00:02 --:--:--  0:00:02 6201k inflated: user_ratedmovies.dat\n",
            "100 17.9M  100 17.9M    0     0  15.5M      0  0:00:01  0:00:01 --:--:-- 15.5M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## recommendation_nn_decision.py\n",
        "## movie problem\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "num_layers = 1\n",
        "activation = 'relu'\n",
        "#k = 20\n",
        "#use_hessian = False\n",
        "num_iters = 100\n",
        "instance_sizes = [100]\n",
        "learning_rate = 1e-4\n",
        "\n",
        "Ps = {}\n",
        "data = {}\n",
        "f_true = {}\n",
        "for num_items in instance_sizes:\n",
        "    with open('diverse_recommendation_data' + '.pickle', 'rb') as f:\n",
        "        # Ps_size takes on the actors 100x500 and data_size takes the users 100x2113\n",
        "        Ps_size, data_size = pickle.load(f)\n",
        "\n",
        "    num_targets = Ps_size[0].shape[1]\n",
        "    num_features = data_size[0].shape[1]\n",
        "    Ps[num_items] = [torch.from_numpy(P).long() for P in Ps_size]\n",
        "    data[num_items] = [torch.from_numpy(x).float() for x in data_size]\n",
        "    w = np.ones(num_targets, dtype=np.float32)\n",
        "    f_true[num_items] = [(P, w) for P in Ps[num_items]]\n",
        "\n",
        "\n",
        "num_repetitions = 0\n",
        "\n",
        "train = {}\n",
        "test = {}\n",
        "for size in instance_sizes:\n",
        "    with open('diverse_recommendation_data' + '.pickle', 'rb') as f:\n",
        "        train[size], test[size] = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "uBQ7Tb0GV0TA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoverageInstanceMultilinear(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Represents a coverage instance with given coverage probabilities\n",
        "    P. Forward pass computes the objective value (if evaluate_forward\n",
        "    is true). Backward computes the gradients w.r.t. decision variables x.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, P):\n",
        "        ctx.save_for_backward(x, P)\n",
        "        out = objective_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        ### print(\"objective_coverage CoverageInstanceMultilinear forward out\", torch.tensor(out).float()) ###\n",
        "        return torch.tensor(out).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        ## print(\"grad_in\", grad_in) ###\n",
        "        x, P = ctx.saved_tensors\n",
        "        grad = gradient_coverage(x.detach().numpy(), P.detach().numpy())\n",
        "        ### print(\"gradient_coverage CoverageInstanceMultilinear backward out\", torch.from_numpy(grad).float()) ###\n",
        "        return torch.from_numpy(grad).float()*grad_in.float(), None"
      ],
      "metadata": {
        "id": "iogXH7gw_cKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "mrcmbbiPYXeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals = np.zeros((num_repetitions+30, len(instance_sizes), len(instance_sizes)))\n",
        "\n",
        "for idx in range(num_repetitions, num_repetitions + 30):\n",
        "\n",
        "    intermediate_size = 200\n",
        "    def make_fc():\n",
        "        if num_layers > 1:\n",
        "            if activation == 'relu':\n",
        "                activation_fn = nn.ReLU\n",
        "            elif activation == 'sigmoid':\n",
        "                activation_fn = nn.Sigmoid\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    'Invalid activation function: ' + str(activation))\n",
        "            net_layers = [\n",
        "                nn.Linear(num_features, intermediate_size), activation_fn()]\n",
        "            for hidden in range(num_layers-2):\n",
        "                net_layers.append(\n",
        "                    nn.Linear(intermediate_size, intermediate_size))\n",
        "                net_layers.append(activation_fn())\n",
        "            net_layers.append(nn.Linear(intermediate_size, num_targets))\n",
        "            net_layers.append(nn.Sigmoid())\n",
        "            return nn.Sequential(*net_layers)\n",
        "        else:\n",
        "            return nn.Sequential(nn.Linear(num_features, num_targets, bias=False)) ##nn.Sequential(nn.Linear(num_features, num_targets), nn.Sigmoid())\n",
        "\n",
        "    # runs the given net on instances of a given size\n",
        "    def eval_opt(net, instances, size):\n",
        "        net.eval()\n",
        "        val = 0.\n",
        "        for i in range(len(instances)):\n",
        "            pred = net(data[size][i])\n",
        "            x = ContinuousOptimizer.apply(pred)\n",
        "            pp, _ = f_true[size][i] #audrey fix\n",
        "            val += objective_coverage(x.detach().numpy(), pp.detach().numpy()) #audrey fix\n",
        "        net.train()\n",
        "        return val/len(instances)\n",
        "\n",
        "    # train a network for each size, and test on each sizes\n",
        "    for train_idx, train_size in enumerate(instance_sizes):\n",
        "        net = make_fc()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "        # training\n",
        "        for t in range(num_iters):\n",
        "            print(f\"Iteration {t}\")\n",
        "\n",
        "            print(\"Get the model predictions...\") ###\n",
        "            i = random.randint(0, 80)\n",
        "            y = data[train_size][0]\n",
        "            ####\n",
        "            init_weights = net.get_submodule(\"0\").get_parameter('weight') ###\n",
        "            #init_bias = net.get_submodule(\"0\").get_parameter('bias') ###\n",
        "            print(\"w / weights\", init_weights)###\n",
        "            #print(\"w / bias\", init_bias)###\n",
        "            ####\n",
        "            pred = net(y)\n",
        "            print(\"theta_hat / pred\", pred) ###\n",
        "            print(\"\\n\")###\n",
        "\n",
        "            print(\"Get the optimal solution to the continous problem...\")\n",
        "            x = ContinuousOptimizer.apply(pred)\n",
        "            print(\"x(theta_hat) / train x\", x)###\n",
        "            print(\"\\n\")###\n",
        "\n",
        "            print(\"Get the objective value and set as the loss...\")\n",
        "            pp, _ = f_true[train_size][0]\n",
        "            loss = -CoverageInstanceMultilinear.apply(x, pp)\n",
        "            print(\"f(x(theta_hat), theta) / train loss\", loss)###\n",
        "            print(\"\\n\")###\n",
        "\n",
        "            print(\"Update model weights based on the computed gradients...\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            print(\"initial weights\", init_weights)###\n",
        "            print(\"weights.grad\", init_weights.grad)###\n",
        "            #print(\"initial bias\", init_bias)###\n",
        "            #print(\"bias.grad\", init_bias.grad)###\n",
        "            print(\"\\n\")###\n",
        "\n",
        "            print(\"Call optimizer.step()...\")\n",
        "            optimizer.step()\n",
        "            weights = net.get_submodule(\"0\").get_parameter('weight') ###\n",
        "            #bias = net.get_submodule(\"0\").get_parameter('bias') ###\n",
        "            print(\"updated weights\", weights)###\n",
        "            print(\"weights.grad\", weights.grad)###\n",
        "            #print(\"updated bias\", bias)###\n",
        "            #print(\"bias.grad\", bias.grad)###\n",
        "            print(\"\\n\")###\n",
        "            break\n",
        "        break\n",
        "        # save learned network state\n",
        "        savepath = '/tmp/net_diffopt_smalllr_{0}_{1}_{2}_{3}.pt'.format(  #audrey fixes /tmp/\n",
        "            train_size, 2, num_layers, idx)\n",
        "        torch.save(net.state_dict(), savepath)\n",
        "        # test on different sizes\n",
        "        for test_idx, test_size in enumerate(instance_sizes):\n",
        "            vals[idx, train_idx, test_idx] = eval_opt(\n",
        "                net, test, test_size)\n",
        "            print(vals[idx, train_idx, test_idx])\n",
        "        # save out values\n",
        "        print(idx, train_size, vals[idx, train_idx])\n",
        "        with open('results_recommendation_' + str(num_layers) + '.pickle', 'wb') as f:\n",
        "            pickle.dump(vals, f)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYHY5R4vwQnx",
        "outputId": "53ec95e9-d4d8-4faa-f79d-b6d8d21fe163"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "Get the model predictions...\n",
            "w / weights Parameter containing:\n",
            "tensor([[ 0.6078],\n",
            "        [ 0.1487],\n",
            "        [-0.2502],\n",
            "        [-0.4783]], requires_grad=True)\n",
            "theta_hat / pred tensor([[ 2.2490,  0.5502, -0.9257, -1.7698],\n",
            "        [ 1.8235,  0.4461, -0.7506, -1.4350],\n",
            "        [ 4.1940,  1.0260, -1.7264, -3.3004],\n",
            "        [ 2.1274,  0.5204, -0.8757, -1.6741],\n",
            "        [ 1.3372,  0.3271, -0.5504, -1.0523],\n",
            "        [ 1.7019,  0.4164, -0.7006, -1.3393],\n",
            "        [ 1.4588,  0.3569, -0.6005, -1.1480],\n",
            "        [ 2.7352,  0.6691, -1.1259, -2.1525],\n",
            "        [ 2.6137,  0.6394, -1.0759, -2.0568],\n",
            "        [ 1.5804,  0.3866, -0.6505, -1.2436],\n",
            "        [ 1.9451,  0.4758, -0.8006, -1.5306],\n",
            "        [ 3.2823,  0.8030, -1.3511, -2.5829],\n",
            "        [ 0.7294,  0.1784, -0.3002, -0.5740],\n",
            "        [ 2.4921,  0.6097, -1.0258, -1.9611],\n",
            "        [ 2.0666,  0.5056, -0.8507, -1.6263],\n",
            "        [ 2.6137,  0.6394, -1.0759, -2.0568],\n",
            "        [ 2.4313,  0.5948, -1.0008, -1.9133],\n",
            "        [ 2.0058,  0.4907, -0.8257, -1.5785],\n",
            "        [ 2.0666,  0.5056, -0.8507, -1.6263],\n",
            "        [ 1.4588,  0.3569, -0.6005, -1.1480],\n",
            "        [ 0.7294,  0.1784, -0.3002, -0.5740],\n",
            "        [ 2.7352,  0.6691, -1.1259, -2.1525],\n",
            "        [ 2.4313,  0.5948, -1.0008, -1.9133],\n",
            "        [ 2.9784,  0.7286, -1.2260, -2.3438],\n",
            "        [ 0.1520,  0.0372, -0.0626, -0.1196],\n",
            "        [ 2.3705,  0.5799, -0.9758, -1.8655]], grad_fn=<MmBackward0>)\n",
            "\n",
            "\n",
            "Get the optimal solution to the continous problem...\n",
            "0 0.0\n",
            "1 -25.59955596923828\n",
            "2 -0.4238263964653015\n",
            "3 -0.42382165789604187\n",
            "4 -0.4238224923610687\n",
            "5 -0.42382150888442993\n",
            "6 -0.42382270097732544\n",
            "7 -0.42382103204727173\n",
            "8 -0.4238223135471344\n",
            "9 -0.4238224923610687\n",
            "x(theta_hat) / train x tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.5250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.5250, 0.0000, 0.0000, 0.0000, 0.9500, 0.0000],\n",
            "       grad_fn=<ContinuousOptimizerBackward>)\n",
            "\n",
            "\n",
            "Get the objective value and set as the loss...\n",
            "f(x(theta_hat), theta) / train loss tensor(-0.9887, grad_fn=<NegBackward0>)\n",
            "\n",
            "\n",
            "Update model weights based on the computed gradients...\n",
            "df(x(theta_hat), theta) / CO grad_output tensor([-2.0000, -1.0000, -3.0000, -1.0000, -0.0000, -0.0000, -0.0000, -2.0000,\n",
            "        -2.0000, -0.0000, -1.0000, -3.0000, -0.0237, -2.0000, -1.0000, -2.0000,\n",
            "        -2.0000, -1.0000, -1.0000, -0.0000, -0.0237, -2.0000, -2.0000, -3.0000,\n",
            "        -0.2256, -2.0000])\n",
            "theta_hat / params tensor([[ 2.2490,  0.5502, -0.9257, -1.7698],\n",
            "        [ 1.8235,  0.4461, -0.7506, -1.4350],\n",
            "        [ 4.1940,  1.0260, -1.7264, -3.3004],\n",
            "        [ 2.1274,  0.5204, -0.8757, -1.6741],\n",
            "        [ 1.3372,  0.3271, -0.5504, -1.0523],\n",
            "        [ 1.7019,  0.4164, -0.7006, -1.3393],\n",
            "        [ 1.4588,  0.3569, -0.6005, -1.1480],\n",
            "        [ 2.7352,  0.6691, -1.1259, -2.1525],\n",
            "        [ 2.6137,  0.6394, -1.0759, -2.0568],\n",
            "        [ 1.5804,  0.3866, -0.6505, -1.2436],\n",
            "        [ 1.9451,  0.4758, -0.8006, -1.5306],\n",
            "        [ 3.2823,  0.8030, -1.3511, -2.5829],\n",
            "        [ 0.7294,  0.1784, -0.3002, -0.5740],\n",
            "        [ 2.4921,  0.6097, -1.0258, -1.9611],\n",
            "        [ 2.0666,  0.5056, -0.8507, -1.6263],\n",
            "        [ 2.6137,  0.6394, -1.0759, -2.0568],\n",
            "        [ 2.4313,  0.5948, -1.0008, -1.9133],\n",
            "        [ 2.0058,  0.4907, -0.8257, -1.5785],\n",
            "        [ 2.0666,  0.5056, -0.8507, -1.6263],\n",
            "        [ 1.4588,  0.3569, -0.6005, -1.1480],\n",
            "        [ 0.7294,  0.1784, -0.3002, -0.5740],\n",
            "        [ 2.7352,  0.6691, -1.1259, -2.1525],\n",
            "        [ 2.4313,  0.5948, -1.0008, -1.9133],\n",
            "        [ 2.9784,  0.7286, -1.2260, -2.3438],\n",
            "        [ 0.1520,  0.0372, -0.0626, -0.1196],\n",
            "        [ 2.3705,  0.5799, -0.9758, -1.8655]], grad_fn=<MmBackward0>)\n",
            "x(theta_hat) / x tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.5250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.5250, 0.0000, 0.0000, 0.0000, 0.9500, 0.0000],\n",
            "       requires_grad=True)\n",
            "dx(theta_hat) / xgrad tensor([3.4832, 2.8243, 6.4958, 3.2950, 2.0711, 2.6360, 2.2594, 4.2364, 4.0481,\n",
            "        2.4477, 3.0125, 5.0837, 0.6589, 3.8598, 3.2008, 4.0481, 3.7657, 3.1067,\n",
            "        3.2008, 2.2594, 0.6589, 4.2364, 3.7657, 4.6129, 0.1979, 3.6715])\n",
            "dx(theta_hat)/dtheta_hat / dxdr (26, 104)\n",
            "dxdr_t torch.Size([104, 26])\n",
            "dfdx*dxdr / CO out tensor([[-2.2791e-03, -5.5432e-03, -9.9315e-03, -1.3192e-02],\n",
            "        [-1.4681e-03, -3.5707e-03, -6.3976e-03, -8.4981e-03],\n",
            "        [-1.6610e-03, -4.0398e-03, -7.2380e-03, -9.6145e-03],\n",
            "        [-1.2060e-03, -2.9331e-03, -5.2552e-03, -6.9807e-03],\n",
            "        [ 5.5862e-05,  1.3586e-04,  2.4343e-04,  3.2335e-04],\n",
            "        [ 3.9903e-05,  9.7050e-05,  1.7388e-04,  2.3097e-04],\n",
            "        [ 4.9290e-05,  1.1988e-04,  2.1479e-04,  2.8531e-04],\n",
            "        [-1.7993e-03, -4.3762e-03, -7.8408e-03, -1.0415e-02],\n",
            "        [-1.8993e-03, -4.6193e-03, -8.2763e-03, -1.0994e-02],\n",
            "        [ 4.4102e-05,  1.0726e-04,  1.9218e-04,  2.5528e-04],\n",
            "        [-1.3507e-03, -3.2851e-03, -5.8858e-03, -7.8183e-03],\n",
            "        [-2.1911e-03, -5.3291e-03, -9.5480e-03, -1.2683e-02],\n",
            "        [ 7.8698e-02,  6.4586e-02, -1.5494e-03, -6.6927e-02],\n",
            "        [-2.0110e-03, -4.8911e-03, -8.7632e-03, -1.1640e-02],\n",
            "        [-1.2506e-03, -3.0418e-03, -5.4498e-03, -7.2392e-03],\n",
            "        [-1.8993e-03, -4.6193e-03, -8.2763e-03, -1.0994e-02],\n",
            "        [-2.0719e-03, -5.0393e-03, -9.0287e-03, -1.1993e-02],\n",
            "        [-1.2987e-03, -3.1587e-03, -5.6594e-03, -7.5176e-03],\n",
            "        [-1.2506e-03, -3.0418e-03, -5.4498e-03, -7.2392e-03],\n",
            "        [ 4.9290e-05,  1.1988e-04,  2.1479e-04,  2.8531e-04],\n",
            "        [ 7.8698e-02,  6.4586e-02, -1.5495e-03, -6.6927e-02],\n",
            "        [-1.7993e-03, -4.3762e-03, -7.8408e-03, -1.0415e-02],\n",
            "        [-2.0719e-03, -5.0393e-03, -9.0287e-03, -1.1993e-02],\n",
            "        [-2.4519e-03, -5.9635e-03, -1.0685e-02, -1.4193e-02],\n",
            "        [ 5.2697e-02,  3.7223e-02, -9.4989e-02, -2.4443e-01],\n",
            "        [-2.1367e-03, -5.1967e-03, -9.3108e-03, -1.2368e-02]])\n",
            "initial weights Parameter containing:\n",
            "tensor([[ 0.6078],\n",
            "        [ 0.1487],\n",
            "        [-0.2502],\n",
            "        [-0.4783]], requires_grad=True)\n",
            "weights.grad tensor([[ 0.0675],\n",
            "        [-0.1630],\n",
            "        [-0.6139],\n",
            "        [-1.0007]])\n",
            "\n",
            "\n",
            "Call optimizer.step()...\n",
            "updated weights Parameter containing:\n",
            "tensor([[ 0.6077],\n",
            "        [ 0.1488],\n",
            "        [-0.2501],\n",
            "        [-0.4782]], requires_grad=True)\n",
            "weights.grad tensor([[ 0.0675],\n",
            "        [-0.1630],\n",
            "        [-0.6139],\n",
            "        [-1.0007]])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check remainder of chain rule\n",
        "\n",
        "y = data[train_size][0]\n",
        "dfdx_dxdr = torch.tensor([[-2.2791e-03, -5.5432e-03, -9.9315e-03, -1.3192e-02],\n",
        "        [-1.4681e-03, -3.5707e-03, -6.3976e-03, -8.4981e-03],\n",
        "        [-1.6610e-03, -4.0398e-03, -7.2380e-03, -9.6145e-03],\n",
        "        [-1.2060e-03, -2.9331e-03, -5.2552e-03, -6.9807e-03],\n",
        "        [ 5.5862e-05,  1.3586e-04,  2.4343e-04,  3.2335e-04],\n",
        "        [ 3.9903e-05,  9.7050e-05,  1.7388e-04,  2.3097e-04],\n",
        "        [ 4.9290e-05,  1.1988e-04,  2.1479e-04,  2.8531e-04],\n",
        "        [-1.7993e-03, -4.3762e-03, -7.8408e-03, -1.0415e-02],\n",
        "        [-1.8993e-03, -4.6193e-03, -8.2763e-03, -1.0994e-02],\n",
        "        [ 4.4102e-05,  1.0726e-04,  1.9218e-04,  2.5528e-04],\n",
        "        [-1.3507e-03, -3.2851e-03, -5.8858e-03, -7.8183e-03],\n",
        "        [-2.1911e-03, -5.3291e-03, -9.5480e-03, -1.2683e-02],\n",
        "        [ 7.8698e-02,  6.4586e-02, -1.5494e-03, -6.6927e-02],\n",
        "        [-2.0110e-03, -4.8911e-03, -8.7632e-03, -1.1640e-02],\n",
        "        [-1.2506e-03, -3.0418e-03, -5.4498e-03, -7.2392e-03],\n",
        "        [-1.8993e-03, -4.6193e-03, -8.2763e-03, -1.0994e-02],\n",
        "        [-2.0719e-03, -5.0393e-03, -9.0287e-03, -1.1993e-02],\n",
        "        [-1.2987e-03, -3.1587e-03, -5.6594e-03, -7.5176e-03],\n",
        "        [-1.2506e-03, -3.0418e-03, -5.4498e-03, -7.2392e-03],\n",
        "        [ 4.9290e-05,  1.1988e-04,  2.1479e-04,  2.8531e-04],\n",
        "        [ 7.8698e-02,  6.4586e-02, -1.5495e-03, -6.6927e-02],\n",
        "        [-1.7993e-03, -4.3762e-03, -7.8408e-03, -1.0415e-02],\n",
        "        [-2.0719e-03, -5.0393e-03, -9.0287e-03, -1.1993e-02],\n",
        "        [-2.4519e-03, -5.9635e-03, -1.0685e-02, -1.4193e-02],\n",
        "        [ 5.2697e-02,  3.7223e-02, -9.4989e-02, -2.4443e-01],\n",
        "        [-2.1367e-03, -5.1967e-03, -9.3108e-03, -1.2368e-02]])"
      ],
      "metadata": {
        "id": "LyGaRPH4E4TV"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yo = dfdx_dxdr*y\n",
        "yo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpZR3HVpFXwO",
        "outputId": "7bbfb228-bfc0-4498-842e-bfb4b5f45eed"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0084, -0.0205, -0.0367, -0.0488],\n",
              "        [-0.0044, -0.0107, -0.0192, -0.0255],\n",
              "        [-0.0115, -0.0279, -0.0499, -0.0663],\n",
              "        [-0.0042, -0.0103, -0.0184, -0.0244],\n",
              "        [ 0.0001,  0.0003,  0.0005,  0.0007],\n",
              "        [ 0.0001,  0.0003,  0.0005,  0.0006],\n",
              "        [ 0.0001,  0.0003,  0.0005,  0.0007],\n",
              "        [-0.0081, -0.0197, -0.0353, -0.0469],\n",
              "        [-0.0082, -0.0199, -0.0356, -0.0473],\n",
              "        [ 0.0001,  0.0003,  0.0005,  0.0007],\n",
              "        [-0.0043, -0.0105, -0.0188, -0.0250],\n",
              "        [-0.0118, -0.0288, -0.0516, -0.0685],\n",
              "        [ 0.0944,  0.0775, -0.0019, -0.0803],\n",
              "        [-0.0082, -0.0201, -0.0359, -0.0477],\n",
              "        [-0.0043, -0.0103, -0.0185, -0.0246],\n",
              "        [-0.0082, -0.0199, -0.0356, -0.0473],\n",
              "        [-0.0083, -0.0202, -0.0361, -0.0480],\n",
              "        [-0.0043, -0.0104, -0.0187, -0.0248],\n",
              "        [-0.0043, -0.0103, -0.0185, -0.0246],\n",
              "        [ 0.0001,  0.0003,  0.0005,  0.0007],\n",
              "        [ 0.0944,  0.0775, -0.0019, -0.0803],\n",
              "        [-0.0081, -0.0197, -0.0353, -0.0469],\n",
              "        [-0.0083, -0.0202, -0.0361, -0.0480],\n",
              "        [-0.0120, -0.0292, -0.0524, -0.0695],\n",
              "        [ 0.0132,  0.0093, -0.0237, -0.0611],\n",
              "        [-0.0083, -0.0203, -0.0363, -0.0482]])"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yo.sum(dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3RRDGwsFuRy",
        "outputId": "ada9524b-24f0-4829-c1d6-00cb953cd8f6"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0675, -0.1630, -0.6139, -1.0007])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geK3Onx8IUW9",
        "outputId": "a98c848e-6a41-485e-b03c-f711b27ec83e"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0675],\n",
              "        [-0.1630],\n",
              "        [-0.6139],\n",
              "        [-1.0007]])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVGgVFgP41aA",
        "outputId": "a592106e-8a74-4556-c9c4-4ba0d4a4f80f"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'state': {0: {'step': tensor(1.), 'exp_avg': tensor([[ 0.0031],\n",
              "           [-0.0094],\n",
              "           [-0.0034],\n",
              "           [-0.0029]]), 'exp_avg_sq': tensor([[9.4479e-07],\n",
              "           [8.8546e-06],\n",
              "           [1.1807e-06],\n",
              "           [8.5244e-07]])}},\n",
              " 'param_groups': [{'lr': 0.0001,\n",
              "   'betas': (0.9, 0.999),\n",
              "   'eps': 1e-08,\n",
              "   'weight_decay': 0,\n",
              "   'amsgrad': False,\n",
              "   'maximize': False,\n",
              "   'foreach': None,\n",
              "   'capturable': False,\n",
              "   'params': [0]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lM5Xkjl0DoK",
        "outputId": "4b47cf38-6cb6-4528-8ca4-dd4f9220d93f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3678, 0.7545, 0.9585, 0.3749],\n",
              "        [0.3766, 0.7160, 0.9286, 0.4366],\n",
              "        [0.3290, 0.8838, 0.9969, 0.1566],\n",
              "        [0.3703, 0.7439, 0.9514, 0.3922],\n",
              "        [0.3867, 0.6678, 0.8709, 0.5095],\n",
              "        [0.3791, 0.7044, 0.9169, 0.4547],\n",
              "        [0.3841, 0.6802, 0.8882, 0.4912],\n",
              "        [0.3579, 0.7940, 0.9780, 0.3091],\n",
              "        [0.3604, 0.7846, 0.9742, 0.3249],\n",
              "        [0.3816, 0.6924, 0.9035, 0.4729],\n",
              "        [0.3741, 0.7274, 0.9387, 0.4187],\n",
              "        [0.3470, 0.8326, 0.9894, 0.2434],\n",
              "        [0.3994, 0.6023, 0.7482, 0.5997],\n",
              "        [0.3629, 0.7749, 0.9697, 0.3412],\n",
              "        [0.3716, 0.7385, 0.9475, 0.4009],\n",
              "        [0.3604, 0.7846, 0.9742, 0.3249],\n",
              "        [0.3641, 0.7699, 0.9672, 0.3495],\n",
              "        [0.3728, 0.7330, 0.9433, 0.4098],\n",
              "        [0.3716, 0.7385, 0.9475, 0.4009],\n",
              "        [0.3841, 0.6802, 0.8882, 0.4912],\n",
              "        [0.3994, 0.6023, 0.7482, 0.5997],\n",
              "        [0.3579, 0.7940, 0.9780, 0.3091],\n",
              "        [0.3641, 0.7699, 0.9672, 0.3495],\n",
              "        [0.3530, 0.8119, 0.9841, 0.2787],\n",
              "        [0.4117, 0.5365, 0.5769, 0.6797],\n",
              "        [0.3653, 0.7649, 0.9645, 0.3579]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights.grad.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReKUAM89zIlP",
        "outputId": "52cd39e4-d8d8-4597-a659-3ab701319f76"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.7900],\n",
              "        [-0.9961],\n",
              "        [-0.6959],\n",
              "        [-2.0019]])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('init_weights:')\n",
        "print(init_weights.grad_fn)\n",
        "print(init_weights.grad_fn.next_functions)\n",
        "print(init_weights.grad_fn.next_functions[0][0].next_functions)\n",
        "print(init_weights.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(init_weights.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "8whOAFV4zKER",
        "outputId": "e81e9835-4b60-4377-eb27-0cba93a8cf5c"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init_weights:\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-fd8aaa49012d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init_weights:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'next_functions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights.grad_fn"
      ],
      "metadata": {
        "id": "WKD7vrJrnvqM"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##predictions match calculation\n",
        "weights = net.get_submodule(\"0\").get_parameter('weight')\n",
        "bias = net.get_submodule(\"0\").get_parameter('bias')\n",
        "print(\"weights\", weights)###\n",
        "print(\"bias\", bias)###\n",
        "print(\"does prediction match pred sigmoid\", torch.sigmoid(data[train_size][0]*weights[0]+bias[0]))\n",
        "#print(\"does prediction match pred\", data[train_size][0]*weights[0]+bias[0])\n",
        "pred = net(data[train_size][0])\n",
        "print(\"pred\", pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpvK59ZUmeD6",
        "outputId": "b5a04d8c-8716-4a4a-ee96-5af7dd9bc01e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights Parameter containing:\n",
            "tensor([[ 0.1862],\n",
            "        [ 0.3591],\n",
            "        [-0.9775],\n",
            "        [-0.8398]], requires_grad=True)\n",
            "bias Parameter containing:\n",
            "tensor([-0.5743,  0.1472,  0.8371, -0.5253], requires_grad=True)\n",
            "does prediction match pred sigmoid tensor([[0.5286],\n",
            "        [0.4961],\n",
            "        [0.6705],\n",
            "        [0.5193],\n",
            "        [0.4589],\n",
            "        [0.4868],\n",
            "        [0.4682],\n",
            "        [0.5655],\n",
            "        [0.5563],\n",
            "        [0.4775],\n",
            "        [0.5054],\n",
            "        [0.6061],\n",
            "        [0.4132],\n",
            "        [0.5471],\n",
            "        [0.5147],\n",
            "        [0.5563],\n",
            "        [0.5425],\n",
            "        [0.5100],\n",
            "        [0.5147],\n",
            "        [0.4682],\n",
            "        [0.4132],\n",
            "        [0.5655],\n",
            "        [0.5425],\n",
            "        [0.5837],\n",
            "        [0.3710],\n",
            "        [0.5379]], grad_fn=<SigmoidBackward0>)\n",
            "pred tensor([[0.5286, 0.8140, 0.0584, 0.0258],\n",
            "        [0.4961, 0.7729, 0.1095, 0.0454],\n",
            "        [0.6705, 0.9325, 0.0027, 0.0018],\n",
            "        [0.5193, 0.8028, 0.0702, 0.0303],\n",
            "        [0.4589, 0.7185, 0.2119, 0.0853],\n",
            "        [0.4868, 0.7600, 0.1301, 0.0533],\n",
            "        [0.4682, 0.7328, 0.1811, 0.0730],\n",
            "        [0.5655, 0.8536, 0.0276, 0.0133],\n",
            "        [0.5563, 0.8444, 0.0334, 0.0157],\n",
            "        [0.4775, 0.7467, 0.1539, 0.0625],\n",
            "        [0.5054, 0.7852, 0.0919, 0.0387],\n",
            "        [0.6061, 0.8896, 0.0116, 0.0063],\n",
            "        [0.4132, 0.6406, 0.4168, 0.1775],\n",
            "        [0.5471, 0.8347, 0.0403, 0.0186],\n",
            "        [0.5147, 0.7971, 0.0768, 0.0329],\n",
            "        [0.5563, 0.8444, 0.0334, 0.0157],\n",
            "        [0.5425, 0.8297, 0.0442, 0.0201],\n",
            "        [0.5100, 0.7912, 0.0840, 0.0357],\n",
            "        [0.5147, 0.7971, 0.0768, 0.0329],\n",
            "        [0.4682, 0.7328, 0.1811, 0.0730],\n",
            "        [0.4132, 0.6406, 0.4168, 0.1775],\n",
            "        [0.5655, 0.8536, 0.0276, 0.0133],\n",
            "        [0.5425, 0.8297, 0.0442, 0.0201],\n",
            "        [0.5837, 0.8707, 0.0188, 0.0096],\n",
            "        [0.3710, 0.5590, 0.6440, 0.3240],\n",
            "        [0.5379, 0.8246, 0.0486, 0.0219]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vPWLyivh8oZ",
        "outputId": "4b94bf03-be41-448a-8ef7-94dc253f0fb6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
            "  return self._grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## recommendation_nn_decision.py\n",
        "## synthetic problem\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# load probability matrix \n",
        "P_list = [\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t0\t,\t0\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t1\t,\t1\t,\t0\t,\t1\t],\n",
        "[\t0\t,\t0\t,\t1\t,\t0\t],\n",
        "[\t0\t,\t1\t,\t0\t,\t1\t],\n",
        "]\n",
        "\n",
        "# load features\n",
        "circuit_km = [\n",
        "3.7, \n",
        "3, \n",
        "6.9, \n",
        "3.5, \n",
        "2.2, \n",
        "2.8, \n",
        "2.4, \n",
        "4.5, \n",
        "4.3, \n",
        "2.6, \n",
        "3.2, \n",
        "5.4, \n",
        "1.2, \n",
        "4.1, \n",
        "3.4, \n",
        "4.3, \n",
        "4, \n",
        "3.3, \n",
        "3.4, \n",
        "2.4, \n",
        "1.2, \n",
        "4.5, \n",
        "4, \n",
        "4.9, \n",
        "0.25, \n",
        "3.9, \n",
        "]\n",
        "y = []\n",
        "for i in circuit_km:\n",
        "  y.append([i])\n",
        "\n",
        "num_layers = 1\n",
        "activation = 'relu'\n",
        "#k = 2\n",
        "#use_hessian = False\n",
        "num_iters = 100\n",
        "instance_sizes = [0]\n",
        "learning_rate = 1e-4\n",
        "\n",
        "Ps = {}\n",
        "data = {}\n",
        "f_true = {}\n",
        "\n",
        "for num_items in instance_sizes:\n",
        "    Ps_size = np.array(P_list)\n",
        "    data_size = np.array(y)\n",
        "\n",
        "    num_targets = Ps_size.shape[1] #500 --> 4\n",
        "    num_features = data_size.shape[1] #2113 --> 1\n",
        "    Ps[num_items] = [torch.from_numpy(Ps_size).long()]\n",
        "    data[num_items] = [torch.from_numpy(data_size).float()]\n",
        "    w = np.ones(num_targets, dtype=np.float32)\n",
        "    f_true[num_items] = [(P, w) for P in Ps[num_items]]\n",
        "  \n",
        "num_repetitions = 0\n",
        "\n",
        "train = {}\n",
        "test = {}\n",
        "for size in instance_sizes:\n",
        "  train[size], test[size] = np.array(P_list), np.array(y)"
      ],
      "metadata": {
        "id": "ehZESn1UV3vk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ps[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GntOoTS9K6B9",
        "outputId": "82166fee-0e69-4bbd-cc3a-79b9d3bbdd2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([26, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}